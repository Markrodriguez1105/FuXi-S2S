{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41827798",
   "metadata": {},
   "source": [
    "# Train FuXi-S2S Bias Correction (PAGASA)\n",
    "\n",
    "This notebook **trains a statistical bias correction model** using the station observations in `data/pagasa` and FuXi-S2S forecast outputs in `output/`.\n",
    "\n",
    "Important: The repository ships an **ONNX inference model** (`model/fuxi_s2s.onnx`). Without the original training code/weights, we cannot retrain the deep network itself here. Instead, we train a lightweight correction layer (linear calibration) that improves station-level forecasts.\n",
    "\n",
    "Outputs:\n",
    "- `train_fuxi/artifacts/bias_correction_params.pkl` (pickled correction parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d662f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ROOT = Path(r\"C:\\Machine Learning\\FuXi-S2S\")\n",
    "os.chdir(REPO_ROOT)\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print('Repo root:', REPO_ROOT)\n",
    "print('Python:', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load PAGASA observations\n",
    "pagasa_file = REPO_ROOT / 'data' / 'pagasa' / 'CBSUA Pili, Camarines Sur Daily data.xlsx'\n",
    "assert pagasa_file.exists(), f'Missing PAGASA file: {pagasa_file}'\n",
    "\n",
    "obs = pd.read_excel(pagasa_file)\n",
    "print('Raw shape:', obs.shape)\n",
    "print('Columns:', list(obs.columns))\n",
    "\n",
    "# Build a Date column from YEAR/MONTH/DAY\n",
    "required = ['YEAR', 'MONTH', 'DAY']\n",
    "for col in required:\n",
    "    if col not in obs.columns:\n",
    "        raise ValueError(f'Missing required column {col} in PAGASA file. Found: {list(obs.columns)}')\n",
    "\n",
    "obs['Date'] = pd.to_datetime(obs[['YEAR', 'MONTH', 'DAY']])\n",
    "obs['date_only'] = obs['Date'].dt.date\n",
    "\n",
    "# Normalize likely observation column names (keep original if present)\n",
    "# Expecting at least RAINFALL and some temperature column(s)\n",
    "print('Date range:', obs['Date'].min(), 'to', obs['Date'].max())\n",
    "\n",
    "display(obs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c860d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Discover available model runs in output/\n",
    "from glob import glob\n",
    "\n",
    "output_root = REPO_ROOT / 'output'\n",
    "assert output_root.exists(), 'Missing output/ directory. Run inference first.'\n",
    "\n",
    "# Find init dates by looking for output/YYYY/YYYYMMDD/member/ directories\n",
    "member_dirs = sorted(Path(output_root).glob('*/*/member'))\n",
    "member_dirs = [d for d in member_dirs if d.is_dir()]\n",
    "\n",
    "# Keep only those that actually contain NetCDF files\n",
    "valid_member_dirs = []\n",
    "for d in member_dirs:\n",
    "    has_flat = any(d.glob('member??_lead??.nc'))\n",
    "    has_sub = any(d.glob('[0-9][0-9]/*.nc'))\n",
    "    if has_flat or has_sub:\n",
    "        valid_member_dirs.append(d)\n",
    "\n",
    "if not valid_member_dirs:\n",
    "    raise RuntimeError('No forecast NetCDF files found in output/. Run inference first.')\n",
    "\n",
    "# Ensure init_date is defined and converted to an integer for comparison\n",
    "init_date = '20000101'  # Default value or replace with a valid date string\n",
    "init_date_int = int(init_date)\n",
    "\n",
    "init_dates = sorted({d.parent.name for d in valid_member_dirs if int(d.parent.name) >= init_date_int})  # d=.../YYYY/YYYYMMDD/member\n",
    "print('Found init dates:', init_dates[:10], ('...' if len(init_dates) > 10 else ''))\n",
    "print('Total init dates:', len(init_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Build training pairs (forecast vs observed) at station location\n",
    "from fuxis2s_model.core.compare import load_fuxi_output, extract_station_forecast, STATIONS\n",
    "\n",
    "station_name = 'CBSUA Pili'\n",
    "station = STATIONS[station_name]\n",
    "station_lat = station['lat']\n",
    "station_lon = station['lon']\n",
    "\n",
    "def list_members_for_init(init_date: str) -> list[int]:\n",
    "    year = init_date[:4]\n",
    "    member_dir = output_root / year / init_date / 'member'\n",
    "    if not member_dir.exists():\n",
    "        return []\n",
    "\n",
    "    files = list(member_dir.glob('member??_lead??.nc'))\n",
    "    members = set()\n",
    "    for p in files:\n",
    "        name = p.name\n",
    "        try:\n",
    "            members.add(int(name.split('_lead')[0].replace('member', '')))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # fallback: subdir structure member/00/*.nc\n",
    "    if not members:\n",
    "        for sub in member_dir.glob('[0-9][0-9]'):\n",
    "            try:\n",
    "                members.add(int(sub.name))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return sorted(members)\n",
    "\n",
    "def ensemble_mean_forecast_df(init_date: str, members: list[int]) -> pd.DataFrame:\n",
    "    per_member = []\n",
    "    for m in members:\n",
    "        try:\n",
    "            da = load_fuxi_output(str(output_root), init_date=init_date, member=m)\n",
    "        except OSError as e:\n",
    "            print(f\"Skipping invalid NetCDF file for member {m} on init_date {init_date}: {e}\")\n",
    "            continue\n",
    "        df = extract_station_forecast(da, lat=station_lat, lon=station_lon, init_date=init_date)\n",
    "        df['member'] = m\n",
    "        per_member.append(df)\n",
    "\n",
    "    all_df = pd.concat(per_member, ignore_index=True)\n",
    "\n",
    "    # Mean over members for each lead time\n",
    "    numeric_cols = all_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    group_cols = ['lead_time_days']\n",
    "    mean_df = all_df.groupby(group_cols, as_index=False)[numeric_cols].mean()\n",
    "\n",
    "    # Restore non-numeric time columns from first member (they're identical per lead time)\n",
    "    time_cols = ['init_time', 'valid_time']\n",
    "    first = all_df.sort_values('member').drop_duplicates('lead_time_days')[['lead_time_days'] + time_cols]\n",
    "    mean_df = mean_df.merge(first, on='lead_time_days', how='left')\n",
    "\n",
    "    mean_df['init_date'] = init_date\n",
    "    return mean_df\n",
    "\n",
    "pairs = []\n",
    "for init_date in init_dates:\n",
    "    members = list_members_for_init(init_date)\n",
    "    if not members:\n",
    "        continue\n",
    "\n",
    "    f = ensemble_mean_forecast_df(init_date, members)\n",
    "    f['date_only'] = pd.to_datetime(f['valid_time']).dt.date\n",
    "\n",
    "    merged = f.merge(obs, on='date_only', how='inner', suffixes=('', '_obs'))\n",
    "    if len(merged) == 0:\n",
    "        continue\n",
    "\n",
    "    pairs.append(merged)\n",
    "\n",
    "if not pairs:\n",
    "    raise RuntimeError('No overlapping forecast/observation dates found. Try running inference for dates covered by PAGASA data.')\n",
    "\n",
    "training_df = pd.concat(pairs, ignore_index=True)\n",
    "print('Training pairs:', len(training_df))\n",
    "print('Columns:', list(training_df.columns)[:25], '...')\n",
    "\n",
    "display(training_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train and save bias corrector\n",
    "from train_fuxi.bias_correction import BiasCorrector\n",
    "\n",
    "# Choose observation columns available in your Excel file.\n",
    "# Common expected names: TMAX, TMIN, RAINFALL, WINDSPEED\n",
    "available = set(training_df.columns)\n",
    "print('Available obs-like columns:', [c for c in ['TMAX','TMIN','RAINFALL','WINDSPEED'] if c in available])\n",
    "\n",
    "mapping = {}\n",
    "if 'TMAX' in available and 't2m_celsius' in available:\n",
    "    mapping['t2m_celsius'] = 'TMAX'\n",
    "if 'RAINFALL' in available and 'tp' in available:\n",
    "    mapping['tp'] = 'RAINFALL'\n",
    "if 'WINDSPEED' in available and 'wind_speed' in available:\n",
    "    mapping['wind_speed'] = 'WINDSPEED'\n",
    "\n",
    "if not mapping:\n",
    "    raise RuntimeError('No mapping could be built. Check your PAGASA column names and forecast columns.')\n",
    "\n",
    "corrector = BiasCorrector().fit(training_df, mapping=mapping)\n",
    "save_path = REPO_ROOT / 'train_fuxi' / 'artifacts' / 'bias_correction_params.pkl'\n",
    "saved = corrector.save(save_path)\n",
    "\n",
    "print('Trained models for:', list(corrector.models.keys()))\n",
    "print('Saved to:', saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Quick evaluation (RMSE before vs after)\n",
    "def rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    m = np.isfinite(a) & np.isfinite(b)\n",
    "    if m.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.sqrt(np.mean((a[m] - b[m])**2)))\n",
    "\n",
    "corrected = corrector.transform(training_df)\n",
    "\n",
    "for forecast_col, obs_col in mapping.items():\n",
    "    raw = rmse(training_df[forecast_col], training_df[obs_col])\n",
    "    cor = rmse(corrected[f'{forecast_col}_corrected'], training_df[obs_col])\n",
    "    print(f'{forecast_col} -> {obs_col}: RMSE raw={raw:.3f}, corrected={cor:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Visualization and summary of all variables\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Summary statistics for all variables\n",
    "summary = training_df.describe(include='all')\n",
    "display(summary)\n",
    "\n",
    "# Visualize distributions of forecast and observed variables\n",
    "forecast_vars = list(mapping.keys())\n",
    "obs_vars = list(mapping.values())\n",
    "all_vars = forecast_vars + obs_vars\n",
    "\n",
    "for var in all_vars:\n",
    "    if var in training_df.columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        label_type = 'Forecasted' if var in forecast_vars else 'Observed' if var in obs_vars else ''\n",
    "        sns.histplot(training_df[var].dropna(), kde=True, bins=30)\n",
    "        plt.title(f'Distribution of {var} ({label_type})')\n",
    "        plt.xlabel(f'{var} value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = training_df[all_vars].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f',\n",
    "            cbar_kws={'label': 'Correlation coefficient'})\n",
    "plt.title('Correlation Heatmap of Forecast and Observed Variables')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92816ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Key variable visualizations: Humidity, Rainfall, Min/Max Temp, Wind (Observed vs Forecasted on same graph, with correct observed columns)\n",
    "key_pairs = [\n",
    "    # (forecast_col, observed_col, label, y_label)\n",
    "    ('tp', 'RAINFALL', 'Rainfall', 'Rainfall (mm)'),\n",
    "    ('t2m_celsius', 'TMAX', 'Max Temperature', 'Temperature (°C)'),\n",
    "    ('wind_speed', 'WINDSPEED', 'Wind Speed', 'Wind Speed (m/s)'),\n",
    "    ('wind_direction', 'WIND DIRECTION', 'Wind Direction', 'Wind Direction (deg)'),\n",
    "    ('t2m_celsius', 'TMIN', 'Min Temperature', 'Temperature (°C)'),\n",
    "]\n",
    "\n",
    "# Use valid_time as x-axis if available and is datetime\n",
    "if 'valid_time' in training_df.columns:\n",
    "    training_df['valid_time'] = pd.to_datetime(training_df['valid_time'])\n",
    "\n",
    "for forecast_col, observed_col, label, y_label in key_pairs:\n",
    "    if forecast_col in training_df.columns and observed_col in training_df.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(training_df['valid_time'], training_df[forecast_col], marker='o', linestyle='-', label=f'{label} (Forecasted)', color='tab:blue')\n",
    "        plt.plot(training_df['valid_time'], training_df[observed_col], marker='s', linestyle='--', label=f'{label} (Observed)', color='tab:orange')\n",
    "        plt.xlabel('Valid Time')\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(f'{label}: Observed vs Forecasted Over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# For variables with only one source (e.g., humidity), plot as before\n",
    "single_vars = [\n",
    "    ('specific_humidity', 'Specific Humidity (Forecasted)'),\n",
    "]\n",
    "for var, label in single_vars:\n",
    "    if var in training_df.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        if 'valid_time' in training_df.columns:\n",
    "            plt.plot(training_df['valid_time'], training_df[var], marker='o', linestyle='-', label=label)\n",
    "            plt.xlabel('Valid Time')\n",
    "            plt.xticks(rotation=45)\n",
    "        else:\n",
    "            sns.histplot(training_df[var].dropna(), kde=True, bins=30)\n",
    "            plt.xlabel(label)\n",
    "        plt.title(f'{label} Over Time' if 'valid_time' in training_df.columns else f'{label} Distribution')\n",
    "        plt.ylabel(label)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a569943",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- If you want this correction applied during storage to MongoDB, we can wire `BiasCorrector.load(...)` into the store pipeline.\n",
    "- For stronger corrections, we can extend the model to include lead-time buckets or quantile mapping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
